{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backoff\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "from openai import OpenAI, RateLimitError\n",
    "import os\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "# embedding model parameters\n",
    "embedding_model = \"text-embedding-ada-002\"\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n",
    "\n",
    "\n",
    "# Define the maximum number of tokens\n",
    "max_tokens = 8000\n",
    "\n",
    "# Assuming tiktoken and the necessary encoding are correctly installed and set up\n",
    "# Define the encoding for the embedding model (replace with the actual encoding you're using)\n",
    "embedding_encoding = \"cl100k_base\"  # Example encoding\n",
    "\n",
    "# Load the JSON data from the file\n",
    "json_df = pd.read_json(\n",
    "    './data/News_Category_Dataset_v3.json', lines=True)\n",
    "\n",
    "# Create a combined column with headline and short_description\n",
    "json_df[\"combined\"] = \"headline: \" + json_df.headline.str.strip() + \\\n",
    "    \"; short_description: \" + json_df.short_description.str.strip()\n",
    "\n",
    "# Get the encoding from tiktoken library\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "# Count the number of tokens using the encoding's encode method\n",
    "json_df[\"n_tokens\"] = json_df.combined.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "# Filter out rows where the token count exceeds the max_tokens limit\n",
    "filtered_json_df = json_df[json_df['n_tokens'] <= max_tokens]\n",
    "\n",
    "# Display the number of lines before and after filtering\n",
    "lines_before = len(json_df)\n",
    "lines_after = len(filtered_json_df)\n",
    "\n",
    "print(\"Lines of text before filtering: \", lines_before)\n",
    "print(\"Lines of text after filtering: \", lines_after)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "def get_embeddings(text, model):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    RateLimitError,\n",
    "    max_time=60,\n",
    "    max_tries=10\n",
    ")\n",
    "def get_embeddings_with_backoff(batch, engine):\n",
    "    return get_embeddings(list_of_text=batch, engine=engine)\n",
    "\n",
    "\n",
    "# Randomly sample 200000 data points\n",
    "df_200k = json_df.sample(200000, random_state=42)\n",
    "\n",
    "# Divide the prompts into 10 batches\n",
    "prompts = df_200k.combined.tolist()\n",
    "prompt_batches = [prompts[i:i+batch_size]\n",
    "                  for i in range(0, len(prompts), batch_size)]\n",
    "\n",
    "# Initialize embedding vector list\n",
    "embeddings = []\n",
    "batch_count = 0\n",
    "\n",
    "# Processing each batch.\n",
    "for batch in prompt_batches:\n",
    "    batch_embeddings = get_embeddings_with_backoff(batch, embedding_model)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    batch_count += 1\n",
    "\n",
    "    # Pause for 65 seconds after every three batches are processed.\n",
    "    if batch_count % 3 == 0:\n",
    "        time.sleep(65)\n",
    "\n",
    "# Add the embedding vectors to the dataframe.\n",
    "df_200k['embedding'] = embeddings\n",
    "\n",
    "# Save the data as a Parquet file.\n",
    "df_200k.to_parquet(\n",
    "    \"./data/News_Category_With_Samples_200k.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>combined</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffpost.com/entry/covid-boosters-...</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Health experts said it is too early to predict...</td>\n",
       "      <td>Carla K. Johnson, AP</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>headline: Over 4 Million Americans Roll Up Sle...</td>\n",
       "      <td>b'[-0.019705895334482193,-0.019851570948958397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffpost.com/entry/american-airlin...</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>He was subdued by passengers and crew when he ...</td>\n",
       "      <td>Mary Papenfuss</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>headline: American Airlines Flyer Charged, Ban...</td>\n",
       "      <td>b'[-0.020039038732647896,-0.016167860478162766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-tweets...</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>\"Until you have a dog you don't understand wha...</td>\n",
       "      <td>Elyse Wanshel</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>headline: 23 Of The Funniest Tweets About Cats...</td>\n",
       "      <td>b'[0.0028837064746767282,0.012555263936519623,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.huffpost.com/entry/funniest-parent...</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>\"Accidentally put grown-up toothpaste on my to...</td>\n",
       "      <td>Caroline Bologna</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>headline: The Funniest Tweets From Parents Thi...</td>\n",
       "      <td>b'[0.0026038987562060356,0.02257823757827282,0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffpost.com/entry/amy-cooper-lose...</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "      <td>Amy Cooper accused investment firm Franklin Te...</td>\n",
       "      <td>Nina Golgowski</td>\n",
       "      <td>2022-09-22</td>\n",
       "      <td>headline: Woman Who Called Cops On Black Bird-...</td>\n",
       "      <td>b'[-0.027758052572607994,-0.023510990664362907...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>headline: RIM CEO Thorsten Heins' 'Significant...</td>\n",
       "      <td>b'[-0.032943569123744965,-0.0286103505641222,-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>headline: Maria Sharapova Stunned By Victoria ...</td>\n",
       "      <td>b'[-0.03897290304303169,-0.0015498248394578695...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>headline: Giants Over Patriots, Jets Over Colt...</td>\n",
       "      <td>b'[-0.028294872492551804,-0.01749710738658905,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>headline: Aldon Smith Arrested: 49ers Lineback...</td>\n",
       "      <td>b'[-0.0012897694250568748,0.00182717340067029,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "      <td>headline: Dwight Howard Rips Teammates After M...</td>\n",
       "      <td>b'[-0.03709607198834419,-0.0059874956496059895...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209527 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "0       https://www.huffpost.com/entry/covid-boosters-...   \n",
       "1       https://www.huffpost.com/entry/american-airlin...   \n",
       "2       https://www.huffpost.com/entry/funniest-tweets...   \n",
       "3       https://www.huffpost.com/entry/funniest-parent...   \n",
       "4       https://www.huffpost.com/entry/amy-cooper-lose...   \n",
       "...                                                   ...   \n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline   category  \\\n",
       "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
       "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
       "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
       "3       The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
       "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
       "...                                                   ...        ...   \n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       Health experts said it is too early to predict...   \n",
       "1       He was subdued by passengers and crew when he ...   \n",
       "2       \"Until you have a dog you don't understand wha...   \n",
       "3       \"Accidentally put grown-up toothpaste on my to...   \n",
       "4       Amy Cooper accused investment firm Franklin Te...   \n",
       "...                                                   ...   \n",
       "209522  Verizon Wireless and AT&T are already promotin...   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...   \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...   \n",
       "209525  CORRECTION: An earlier version of this story i...   \n",
       "209526  The five-time all-star center tore into his te...   \n",
       "\n",
       "                     authors       date  \\\n",
       "0       Carla K. Johnson, AP 2022-09-23   \n",
       "1             Mary Papenfuss 2022-09-23   \n",
       "2              Elyse Wanshel 2022-09-23   \n",
       "3           Caroline Bologna 2022-09-23   \n",
       "4             Nina Golgowski 2022-09-22   \n",
       "...                      ...        ...   \n",
       "209522      Reuters, Reuters 2012-01-28   \n",
       "209523                       2012-01-28   \n",
       "209524                       2012-01-28   \n",
       "209525                       2012-01-28   \n",
       "209526                       2012-01-28   \n",
       "\n",
       "                                                 combined  \\\n",
       "0       headline: Over 4 Million Americans Roll Up Sle...   \n",
       "1       headline: American Airlines Flyer Charged, Ban...   \n",
       "2       headline: 23 Of The Funniest Tweets About Cats...   \n",
       "3       headline: The Funniest Tweets From Parents Thi...   \n",
       "4       headline: Woman Who Called Cops On Black Bird-...   \n",
       "...                                                   ...   \n",
       "209522  headline: RIM CEO Thorsten Heins' 'Significant...   \n",
       "209523  headline: Maria Sharapova Stunned By Victoria ...   \n",
       "209524  headline: Giants Over Patriots, Jets Over Colt...   \n",
       "209525  headline: Aldon Smith Arrested: 49ers Lineback...   \n",
       "209526  headline: Dwight Howard Rips Teammates After M...   \n",
       "\n",
       "                                                embedding  \n",
       "0       b'[-0.019705895334482193,-0.019851570948958397...  \n",
       "1       b'[-0.020039038732647896,-0.016167860478162766...  \n",
       "2       b'[0.0028837064746767282,0.012555263936519623,...  \n",
       "3       b'[0.0026038987562060356,0.02257823757827282,0...  \n",
       "4       b'[-0.027758052572607994,-0.023510990664362907...  \n",
       "...                                                   ...  \n",
       "209522  b'[-0.032943569123744965,-0.0286103505641222,-...  \n",
       "209523  b'[-0.03897290304303169,-0.0015498248394578695...  \n",
       "209524  b'[-0.028294872492551804,-0.01749710738658905,...  \n",
       "209525  b'[-0.0012897694250568748,0.00182717340067029,...  \n",
       "209526  b'[-0.03709607198834419,-0.0059874956496059895...  \n",
       "\n",
       "[209527 rows x 8 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = './data/News_Category_With_Samples_200k.parquet'\n",
    "\n",
    "df = pd.read_parquet(parquet_file_path)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.54      0.48      0.51       462\n",
      "ARTS & CULTURE       0.57      0.26      0.36       394\n",
      "  BLACK VOICES       0.56      0.44      0.49      1424\n",
      "      BUSINESS       0.57      0.56      0.57      1755\n",
      "       COLLEGE       0.58      0.51      0.54       368\n",
      "        COMEDY       0.58      0.46      0.52      1613\n",
      "         CRIME       0.58      0.67      0.62      1047\n",
      "CULTURE & ARTS       0.82      0.35      0.49       319\n",
      "       DIVORCE       0.82      0.73      0.77      1069\n",
      "     EDUCATION       0.51      0.39      0.44       309\n",
      " ENTERTAINMENT       0.68      0.82      0.75      5189\n",
      "   ENVIRONMENT       0.71      0.35      0.47       470\n",
      "         FIFTY       0.53      0.27      0.36       420\n",
      "  FOOD & DRINK       0.66      0.86      0.75      1910\n",
      "     GOOD NEWS       0.42      0.30      0.35       381\n",
      "         GREEN       0.47      0.52      0.49       740\n",
      "HEALTHY LIVING       0.60      0.36      0.45      1951\n",
      " HOME & LIVING       0.80      0.80      0.80      1346\n",
      "        IMPACT       0.50      0.36      0.42      1042\n",
      " LATINO VOICES       0.63      0.38      0.48       349\n",
      "         MEDIA       0.65      0.47      0.54       886\n",
      "         MONEY       0.58      0.43      0.50       538\n",
      "     PARENTING       0.63      0.77      0.69      2657\n",
      "       PARENTS       0.56      0.34      0.42      1138\n",
      "      POLITICS       0.75      0.87      0.81     10708\n",
      "  QUEER VOICES       0.79      0.73      0.76      1878\n",
      "      RELIGION       0.62      0.59      0.60       775\n",
      "       SCIENCE       0.68      0.57      0.62       639\n",
      "        SPORTS       0.77      0.85      0.81      1571\n",
      "         STYLE       0.67      0.45      0.54       670\n",
      "STYLE & BEAUTY       0.82      0.88      0.85      2916\n",
      "         TASTE       0.51      0.12      0.19       635\n",
      "          TECH       0.61      0.57      0.59       613\n",
      " THE WORLDPOST       0.59      0.51      0.55      1104\n",
      "        TRAVEL       0.77      0.86      0.82      2950\n",
      "     U.S. NEWS       0.61      0.14      0.22       436\n",
      "      WEDDINGS       0.85      0.80      0.82      1134\n",
      "    WEIRD NEWS       0.52      0.44      0.48       831\n",
      "      WELLNESS       0.68      0.86      0.76      5419\n",
      "         WOMEN       0.46      0.36      0.41      1046\n",
      "    WORLD NEWS       0.54      0.40      0.46       993\n",
      "     WORLDPOST       0.67      0.62      0.64       764\n",
      "\n",
      "      accuracy                           0.69     62859\n",
      "     macro avg       0.63      0.54      0.56     62859\n",
      "  weighted avg       0.67      0.69      0.67     62859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# load data\n",
    "training_data = pd.read_parquet(\"./data/News_Category_With_Samples_200k.parquet\")\n",
    "\n",
    "# random sampling\n",
    "df = training_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Since the embeddings may be saved as a list in string form, we need to convert it into a numpy array.\n",
    "def embeddings_to_floats(embedding_str):\n",
    "    # Use json.loads to safely convert a list of strings into a numpy array.\n",
    "    return np.array(json.loads(embedding_str))\n",
    "\n",
    "# Apply the conversion function to each embedding.\n",
    "# Note: If the dataset is very large, this may consume a significant amount of memory, and batch processing may be necessary.\n",
    "X = np.stack(df['embedding'].apply(embeddings_to_floats).values)\n",
    "y = df['category'].values\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# train a random forest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# generate a classification report\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# load data\n",
    "training_data = pd.read_parquet(\"./data/News_Category_With_Samples_200k.parquet\")\n",
    "\n",
    "# random sampling\n",
    "df = training_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Since the embeddings may be saved as a list in string form, we need to convert it into a numpy array.\n",
    "def embeddings_to_floats(embedding_str):\n",
    "    # Use json.loads to safely convert a list of strings into a numpy array.\n",
    "    return np.array(json.loads(embedding_str))\n",
    "\n",
    "# Apply the conversion function to each embedding.\n",
    "# Note: If the dataset is very large, this may consume a significant amount of memory, and batch processing may be necessary.\n",
    "X = np.stack(df['embedding'].apply(embeddings_to_floats).values)\n",
    "y = df['category'].values\n",
    "\n",
    "# split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train a logistic regression model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# generate a classification report\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_aesthetics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
