 Hello and welcome to ITI's podcast, Download on Tech. I'm Jason Oxman. ITI is the global trade association of the technology industry, representing the world's technology innovators. When you listen to episodes of Download on Tech, you'll hear from leaders at the most innovative technology companies, policymakers, and influencers from around the world. Our guests will share their perspective on the intersection of innovation and public policy. Today, I'm very pleased to be joined by Tris Warckenton. He's director of product management at Google DeepMind, a cutting-edge AI research lab. In this role at Google DeepMind, Tris is spearheading AI research that focuses on advancing the next generation of AI to achieve greater levels of innovation and benefits to society. Now, prior to joining Google DeepMind, Tris was at the helm of the product management function for the brain team within Google Research, where he led a team of AI researchers from around the globe. Tris, welcome to the show. It's an absolute pleasure to be here with you. Thank you for having me so much. I'm really excited to share some of the cool stuff that we're working on in AI and thankful for the opportunity to talk to ITI's listeners and regulators around the globe. We are so fortunate to have you because regulators around the globe are, in fact, paying very close attention to what's going on in AI. I would say it's AI Week, AI Month, AI Year, all of the discussions around the world here in Washington, in the UK, it's taking place in Brussels, at the EU. So having you here with us today to hear your thoughts about AI are incredibly important given that you are a legitimate scientist and regulators can learn a lot from you. So before we get into the policy discussion and talking about what's going on at Google DeepMind that we all need to know about, I wanted to ask if you could tell our listeners something kind of fun or maybe a historical fact about AI that they may not know. Ooh, that's such an interesting question. I think that there are so many things that are mythological to some extent in AI. Many people know that AI started back in the 60s and was obviously very, very different. We've had multiple generations of thinking about AI. One thing that I think is an interesting and historical fun fact about AI is that deep neural nets actually are completely not a new phenomenon. They were something that's been around for at least 35 years as a mathematical concept and as a functional concept. And really, it's been about the scale and the way that we get that in touch with users and interesting product applications that has had a transformative impact on your life. Very interesting. And of course, the fact that you mentioned that it's been around for decades is something I think people forget. There's this tendency to believe that AI just exploded on the scene when we started talking about foundation models six months ago. But this kind of software, these algorithms, all the stuff that's behind AI has been around for a very long time. And that's a very interesting thing for us to remember when we talk about how AI should be regulated. Let's talk about AI and what it is and maybe even what it isn't. Can you give us a kind of a high level explanation of how AI algorithms like deep learning, all these things we're reading about in the headlines, how do they work and how do they learn from data? Yes, absolutely. I think one thing that I'll say that's actually maybe a surprise to you and the other listeners is that I know it feels like forever since the AI revolution has sort of hit the public consciousness. In fact, it hasn't even been really six months. It's been almost a year since Chachi PT was released, a year and a half since Lambda sentience kerfuffle, for example. And I think that the work goes back way, way before that. So let's talk quickly about how they work. But we'd love to dig in as well and sort of how the public sentiment is changing and shifting around AI and what that means for policy workers around the globe. So how does AI work fundamentally? Let's just talk about language models really quickly. Language models are what's known as a sequence model, and they're trained to find patterns in data. So one of the most important things to realize about AI is that it is absolutely not magic. I know that it can feel magical at times, but it is just about the mathematical relationships between data that you might find. So, for example, take the phrase, the capital of France is blank. Right. You need to fill in Paris from your own knowledge, your own memory. And that's fundamentally the same way that a next token prediction large language model works as well. And it comes to that conclusion by looking at other instances of similar word patterns. So you can think of a large language model sort of in three fundamental simple parts. One is the weights, what we call the weights. And that is sort of like the the knowledge and correlation matrix between all of the words that it has seen during its training phase. And then you have something called the context window. That's the second element. And that is like a short term memory for a language model. And it just remembers some number of words or tokens, essentially, that that are inside the context window. So whatever it read last, the last 10,000 words of our conversation, things like that. And then it does next token prediction. And that is just like the Paris example. So when you look at it in that light, you'll see that it's not really quite as strange and hard to understand as it sort of is made out to be. It really is just creating one word at a time the same way that I am right now. That's really interesting. So it's a really highly advanced sentence completion in the example you gave. But I know that AI algorithms are doing more with the kind of the fundamental blocks of AI systems. So you're talking about the ones we're most familiar with, the chat GPT type models, which are predicting where things are heading based on what I'm typing in or the inquiry that I'm providing to it. But can you tell us a little bit more about some of the other fundamental building blocks even beyond these kind of sentence completion examples that you're giving? How do all the building blocks of AI systems come together to make decisions about not only what's coming next in a sentence, but more things that we hear about, the applications we hear about that make our lives easier and better? What are the building blocks behind those AI systems? Yeah, I love this question and this framing because it's really hard sometimes to see how something like a next word prediction, autocomplete algorithm, sort of as you said, could be pieced into a system that does this. And of course, it starts relatively simply as a next word prediction algorithm. But we have a wide variety of interesting research and methods that enable us to interface with a wide variety of technology applications. So I'll give an example, perhaps here that will be illustrative. Here at Google, we have an AI product called Google Bard, which I was lucky enough to actually work on in its earliest phases. And I even happened to name the product Bard. So if you hate the name Bard, I'm the guy to blame, but please don't email me. You can email Jason and complain to Jason and it'll work out. When you look at Bard, it's actually an amalgamation of a wide variety of capabilities and what you're calling fundamental building blocks. First you have the large language model itself. That is just the pre-trained next token prediction concept. On top of that, we layer a wide variety of control mechanisms because safety and responsibility are really at the core of what we want to do. So how do we do that? Well, we do a lot of fine tuning. What fine tuning does is we try to find negative examples, things that might go wrong. We try to instruct the model not to behave in ways that we find potentially difficult or harmful or something that might yield a good answer for a novel, but not for a factual response, for example. And we do that for a wide variety of tasks, trying to incorporate the cultures and perspectives of many people across the globe. And that's one of the things that I think is so exciting about this generation of AI is that everybody has a chance to participate, that you can use Bard, you can go and learn about AI and build things with AI for free right now on Google and many other places as well. And I think that bringing more people in is going to broaden our ability to build those fundamental building blocks into a safety tune. Then the third element of this is actually interconnection to services that make our lives easier. You may have seen, if you haven't, I recommend going to check out Bard can now use Google flights or Google hotels. It can use maps and a variety of other things, including YouTube from across the Google ecosystem in order to provide you just that right route for your hike or the right hotel for your trip to London next week. And a lot of these applications that you're talking about are built on Google technology, but you're also very importantly making these foundation model capabilities available to third parties to create the next exciting AI iteration. And I wanted to ask you, Tris, for your approach to what we hear a lot about, the risks of foundation models. You alluded earlier to the regulatory audience that's out there that's asking a lot of questions about how we can manage risk in the context of foundation models. And Google has been a leader in educating policymakers talking about it. I participated yesterday in a session up on Capitol Hill led by Senate Majority Leader Schumer of New York. And there was a Google representative there as well sharing her views. So this has been a great part of Google's efforts. You're also participating in ITI's AI Futures Initiative, which is a task force of AI policy experts that's trying to address these emerging questions about AI. And the task force recently published a foundation model paper that Google was very heavily involved in. Can you share more about Google's approach to risk management in the context of these foundation models of AI that you're talking about? Absolutely. First, want to say thank you, because I think we're very excited about the AI Futures Initiative. We think that there's a lot of potential to broaden our perspective and learn from policymakers, regulators and industry thought leaders worldwide. And I think this is a huge step. So really grateful for that. In terms of safety and policy, I think it's really important to remember that we're very early in this adventure of AI. I like to tell people that we're not in the middle of the AI revolution. We might not even be in the middle of the beginning of the AI revolution. I think we're almost at the beginning of the beginning, even though we've been working on AI to some extent for decades. This is a new era and we have fundamental things that we need to figure out about AI. So most important when thinking about safety and risk management is realizing that we don't know what the future is, but that we need something that will be a consolidated viewpoint across policymakers, across regulatory bodies that can foster responsible innovation across a wide variety of companies. And that's why Google has supported standards based approaches to make sure that we're developing responsible, safe foundation models with appropriate controls and auditing and things like that, as you may have seen from the recent executive order. And actually, speaking of that executive order, which came out of the Biden administration in the waning days of October, the executive order makes a lot of references to voluntary commitments that companies have made to the White House to ensure the safe and responsible deployment of AI. Google was one of the very first companies to sign up for those voluntary commitments from the White House earlier this year. I want to ask you about how those commitments are integrated into your AI initiatives and what Google is focused on related to the deployment of a safe and responsible AI. It's a wonderful question. I actually am very excited about the White House commitments and about some of the opportunity that we have in the regulatory area, not because I think that regulation is necessarily an accelerator for all innovation, but because this is something that we've been trying to establish as standards across the industry for well over five years now. In 2018, we published our responsible principles and you can find those. Just go search for Google responsible principles. And they are principles that guide our product development and our research areas. And we take those very seriously. It's something where we're trying to make safe, beneficial AI that can benefit everybody in humanity, as opposed to something that works for big tech or works for a very particular technology enlightened audiences. So this next wave of policy and regulation, I think very much tails on to that initiative and that sort of lens we took starting.