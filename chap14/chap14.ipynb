{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY=<PUT_YOUR_API_KEY_HERE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "En tant qu'expert en apprentissage automatique, veuillez présenter le principe de CNN ?\n",
      "\n",
      "\n",
      "Le réseau neuronal convolutif (CNN) est une technique d'apprentissage automatique qui combine l'apprentissage profond et le traitement des images. Les CNN sont composés de couches de neurones connectés entre eux, qui sont utilisés pour extraire des caractéristiques des images. Les couches d'entrée sont connectées à des couches de convolution, qui appliquent des filtres à l'image pour extraire des caractéristiques. Ces caractéristiques sont ensuite passées à des couches de max-pooling, qui réduisent la taille des images tout en conservant les caractéristiques importantes. Finalement, les données sont envoyées à des couches entièrement connectées, qui sont utilisées pour classer l'image.\n",
      "\n",
      "\n",
      "Convolutional Neural Network (CNN) is a machine learning technique that combines deep learning and image processing. CNNs are composed of layers of neurons connected to each other, which are used to extract features from images. The input layers are connected to convolution layers, which apply filters to the image to extract features. These features are then passed to max-pooling layers, which reduce the size of the images while keeping the important features. Finally, the data is sent to fully connected layers, which are used to classify the image.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import openai\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "llm = openai.OpenAI(model=\"text-davinci-003\", max_tokens=2048, temperature=0.5)\n",
    "\n",
    "en_to_fr_prompt = PromptTemplate(\n",
    "    template=\"Please translate the following sentence into French: \\n\\n {question}?\", input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template = \"{english_question}\", input_variables=[\"english_question\"]\n",
    ")\n",
    "\n",
    "fr_to_en_prompt = PromptTemplate(\n",
    "    input_variables=[\"english_answer\"],\n",
    "    template=\"Please translate the following paragraph into English： \\n\\n{english_answer}?\",\n",
    ")\n",
    "\n",
    "question_translate_chain = LLMChain(llm=llm, prompt=en_to_fr_prompt, output_key=\"english_question\")\n",
    "english = question_translate_chain.run(question=\"As an expert in machine learning, please introduce the principle of CNN.\")\n",
    "print(english)\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=question_prompt, output_key=\"english_answer\")\n",
    "english_answer = qa_chain.run(english_question=english)\n",
    "print(english_answer)\n",
    "\n",
    "answer_translate_chain = LLMChain(llm=llm, prompt=fr_to_en_prompt)\n",
    "answer = answer_translate_chain.run(english_answer=english_answer)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "En tant qu'expert en apprentissage automatique, veuillez présenter le principe du Réseau de Neurones Convolutifs (CNN) ?\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Le Réseau de Neurones Convolutifs (CNN) est une technique d'apprentissage automatique qui est largement utilisée pour la reconnaissance d'images et le traitement du langage naturel. Il est basé sur le concept de couches de neurones artificiels qui sont connectées entre elles et qui sont entraînées pour effectuer des tâches spécifiques.\n",
      "\n",
      "Les CNNs sont composés de plusieurs couches de neurones qui sont connectées entre elles. Chaque couche est responsable de la transformation des données d'entrée en une représentation plus abstraite. Les couches de convolution appliquent des filtres à l'image d'entrée qui extraient des caractéristiques spécifiques. Les couches de pooling combinent ces caractéristiques et les réduisent à une taille plus petite. Les couches entièrement connectées sont ensuite utilisées pour effectuer la classification des images.\u001b[0m\n",
      "\u001b[38;5;200m\u001b[1;3m\n",
      "\n",
      "The Convolutional Neural Network (CNN) is a machine learning technique that is widely used for image recognition and natural language processing. It is based on the concept of artificial neuron layers that are connected to each other and are trained to perform specific tasks. \n",
      "\n",
      "CNNs consist of multiple layers of neurons that are connected to each other. Each layer is responsible for transforming the input data into a more abstract representation. The convolutional layers apply filters to the input image which extract specific features. The pooling layers combine these features and reduce them to a smaller size. The fully connected layers are then used to perform the classification of the images.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "The Convolutional Neural Network (CNN) is a machine learning technique that is widely used for image recognition and natural language processing. It is based on the concept of artificial neuron layers that are connected to each other and are trained to perform specific tasks. \n",
      "\n",
      "CNNs consist of multiple layers of neurons that are connected to each other. Each layer is responsible for transforming the input data into a more abstract representation. The convolutional layers apply filters to the input image which extract specific features. The pooling layers combine these features and reduce them to a smaller size. The fully connected layers are then used to perform the classification of the images.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "french_qa_chain = SimpleSequentialChain(\n",
    "    chains=[question_translate_chain, qa_chain, answer_translate_chain], input_key=\"question\",\n",
    "    verbose=True)\n",
    "answer = french_qa_chain.run(question=\"En tant qu'expert en apprentissage automatique, veuillez présenter le principe du CNN.\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "The Denver Broncos have won 3 Super Bowl championships, while the St. Louis Rams have won 1.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "q1_prompt = PromptTemplate(\n",
    "    input_variables=[\"year1\"],\n",
    "    template=\"Which team won the Super Bowl in {year1}? Only the team name is required.\"\n",
    ")\n",
    "q2_prompt = PromptTemplate(\n",
    "    input_variables=[\"year2\"],\n",
    "    template=\"Which team won the Super Bowl in {year2}? Only the team name is required.\"\n",
    ")\n",
    "q3_prompt = PromptTemplate(\n",
    "    input_variables=[\"team1\", \"team2\"],\n",
    "    template=\"Which team, {team1} or {team2}, has won more Super Bowl championships?\"\n",
    ")\n",
    "chain1 = LLMChain(llm=llm, prompt=q1_prompt, output_key=\"team1\")\n",
    "chain2 = LLMChain(llm=llm, prompt=q2_prompt, output_key=\"team2\")\n",
    "chain3 = LLMChain(llm=llm, prompt=q3_prompt)\n",
    "\n",
    "sequential_chain = SequentialChain(chains=[chain1, chain2, chain3], input_variables=[\"year1\", \"year2\"], verbose=True)\n",
    "answer = sequential_chain.run(year1=2016, year2=2000)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/langchain/utils/utils.py:159: UserWarning: WARNING! stop is not default parameter.\n",
      "                stop was transferred to model_kwargs.\n",
      "                Please confirm that stop is what you intended.\n",
      "  warnings.warn(\n",
      "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/langchain/utils/utils.py:159: UserWarning: WARNING! stop is not default parameter.\n",
      "                stop was transferred to model_kwargs.\n",
      "                Please confirm that stop is what you intended.\n",
      "  warnings.warn(\n",
      "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/langchain/utils/utils.py:159: UserWarning: WARNING! stop is not default parameter.\n",
      "                stop was transferred to model_kwargs.\n",
      "                Please confirm that stop is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "def earth_to_mars_time(earth_seconds):\n",
      "    # Length of a Martian sol in Earth seconds\n",
      "    mars_sol_length = 88775.244\n",
      "\n",
      "    # Convert Earth seconds to Mars seconds\n",
      "    mars_seconds = earth_seconds * mars_sol_length / 86400\n",
      "\n",
      "    # Calculate Mars time components\n",
      "    sols = int(mars_seconds // mars_sol_length)\n",
      "    mars_seconds %= mars_sol_length\n",
      "    hours = int(mars_seconds // 3600)\n",
      "    mars_seconds %= 3600\n",
      "    minutes = int(mars_seconds // 60)\n",
      "    seconds = int(mars_seconds % 60)\n",
      "\n",
      "    return sols, hours, minutes, seconds\n",
      "class TestEarthToMarsTime(unittest.TestCase):\n",
      "    \"\"\"\n",
      "    This class contains our unit tests for the earth_to_mars_time function.\n",
      "    \"\"\"\n",
      "\n",
      "    def test_normal_cases(self):\n",
      "        \"\"\"\n",
      "        This test case tests the function's behavior for a variety of normal inputs.\n",
      "        \"\"\"\n",
      "\n",
      "        # Test 1: The function is called with a positive integer\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(1000000000),  # input\n",
      "            (1102, 12, 53, 20),  # expected output\n",
      "        )\n",
      "\n",
      "        # Test 2: The function is called with a negative integer\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(-1000000000),  # input\n",
      "            (-1102, -12, -53, -20),  # expected output\n",
      "        )\n",
      "\n",
      "        # Test 3: The function is called with zero\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(0),  # input\n",
      "            (0, 0, 0, 0),  # expected output\n",
      "        )\n",
      "\n",
      "    def test_edge_cases(self):\n",
      "        \"\"\"\n",
      "        This test case tests the function's behavior for a variety of edge inputs.\n",
      "        \"\"\"\n",
      "\n",
      "        # Test 4: The function is called with a very large integer\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(1000000000000000000),  # input\n",
      "            (1112581312, 12, 53, 20),  # expected output\n",
      "        )\n",
      "\n",
      "        # Test 5: The function is called with a very small integer\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(-1000000000000000000),  # input\n",
      "            (-1112581312, -12, -53, -20),  # expected output\n",
      "        )\n",
      "\n",
      "        # Test 6: The function is called with a non-integer value\n",
      "        self.assertEqual(\n",
      "            earth_to_mars_time(0.5),  # input\n",
      "            (0, 0, 0, 0.5),  # expected output\n",
      "        )\n",
      "\n",
      "    def test_expected_failures(self):\n",
      "        \"\"\"\n",
      "        This test case tests the function's behavior for a variety of expected failure inputs.\n",
      "        \"\"\"\n",
      "\n",
      "        # Test 7: The function is called with a value that is not a number\n",
      "        with self.assertRaises(TypeError):\n",
      "            earth_to_mars_time(\"not a number\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    unittest.main()\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "def write_unit_test(function_to_test, unit_test_package = \"unittest\"):\n",
    "\n",
    "    explain_code = \"\"\"\"# How to write great unit tests with {unit_test_package}\n",
    "\n",
    "    In this advanced tutorial for experts, we'll use Python 3.10 and `{unit_test_package}` to write a suite of unit tests to verify the behavior of the following function.\n",
    "    ```python\n",
    "    {function_to_test}\n",
    "    ```\n",
    "\n",
    "    Before writing any unit tests, let's review what each element of the function is doing exactly and what the author's intentions may have been.\n",
    "    - First,\"\"\"\n",
    "\n",
    "    explain_code_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\"],\n",
    "        template=explain_code\n",
    "    )\n",
    "    explain_code_llm = openai.OpenAI(model=\"text-davinci-002\", temperature=0.4, max_tokens=1000,\n",
    "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])\n",
    "    explain_code_step = LLMChain(llm=explain_code_llm, prompt=explain_code_template, output_key=\"code_explaination\")\n",
    "\n",
    "    test_plan = \"\"\"\n",
    "\n",
    "    A good unit test suite should aim to:\n",
    "    - Test the function's behavior for a wide range of possible inputs\n",
    "    - Test edge cases that the author may not have foreseen\n",
    "    - Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
    "    - Be easy to read and understand, with clean code and descriptive names\n",
    "    - Be deterministic, so that the tests always pass or fail in the same way\n",
    "\n",
    "    `{unit_test_package}` has many convenient features that make it easy to write and maintain unit tests. We'll use them to write unit tests for the function above.\n",
    "\n",
    "    For this particular function, we'll want our unit tests to handle the following diverse scenarios (and under each scenario, we include a few examples as sub-bullets):\n",
    "    -\"\"\"\n",
    "    test_plan_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\"],\n",
    "        template= explain_code + \"{code_explaination}\" + test_plan\n",
    "    )\n",
    "    test_plan_llm = openai.OpenAI(model=\"text-davinci-002\", temperature=0.4, max_tokens=1000,\n",
    "            top_p=1, stop=[\"\\n\\n\", \"\\n\\t\\n\", \"\\n    \\n\"])\n",
    "    test_plan_step = LLMChain(llm=test_plan_llm, prompt=test_plan_template, output_key=\"test_plan\")\n",
    "\n",
    "    prompt_to_generate_the_unit_test = \"\"\"\n",
    "\n",
    "Before going into the individual tests, let's first look at the complete suite of unit tests as a cohesive whole. We've added helpful comments to explain what each line does.\n",
    "```python\n",
    "import {unit_test_package}  # used for our unit tests\n",
    "\n",
    "{function_to_test}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    unit_test_template = PromptTemplate(\n",
    "        input_variables=[\"unit_test_package\", \"function_to_test\", \"code_explaination\", \"test_plan\"],\n",
    "        template= explain_code + \"{code_explaination}\" + test_plan + \"{test_plan}\" + prompt_to_generate_the_unit_test\n",
    "    )\n",
    "    unit_test_llm = openai.OpenAI(model=\"text-davinci-002\", temperature=0.4, max_tokens=1000, stop=\"```\")\n",
    "    unit_test_step = LLMChain(llm=unit_test_llm, prompt=unit_test_template, output_key=\"unit_test\")\n",
    "\n",
    "    sequential_chain = SequentialChain(chains=[explain_code_step, test_plan_step, unit_test_step],\n",
    "                                    input_variables=[\"unit_test_package\", \"function_to_test\"], verbose=True)\n",
    "    answer = sequential_chain.run(unit_test_package=unit_test_package, function_to_test=function_to_test)\n",
    "    return answer\n",
    "\n",
    "code = \"\"\"\n",
    "def earth_to_mars_time(earth_seconds):\n",
    "    # Length of a Martian sol in Earth seconds\n",
    "    mars_sol_length = 88775.244\n",
    "\n",
    "    # Convert Earth seconds to Mars seconds\n",
    "    mars_seconds = earth_seconds * mars_sol_length / 86400\n",
    "\n",
    "    # Calculate Mars time components\n",
    "    sols = int(mars_seconds // mars_sol_length)\n",
    "    mars_seconds %= mars_sol_length\n",
    "    hours = int(mars_seconds // 3600)\n",
    "    mars_seconds %= 3600\n",
    "    minutes = int(mars_seconds // 60)\n",
    "    seconds = int(mars_seconds % 60)\n",
    "\n",
    "    return sols, hours, minutes, seconds\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "\n",
    "def write_unit_test_automatically(code, retry=3):\n",
    "    unit_test_code = write_unit_test(code)\n",
    "    all_code = code + unit_test_code\n",
    "    tried = 0\n",
    "    while tried < retry:\n",
    "        try:\n",
    "            ast.parse(all_code)\n",
    "            return all_code\n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error in generated code: {e}\")\n",
    "            all_code = code + write_unit_test(code)\n",
    "            tried += 1\n",
    "\n",
    "print(write_unit_test_automatically(code))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_aesthetics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
