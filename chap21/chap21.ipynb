{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%env OPENAI_API_KEY=<PUT_YOUR_API_KEY_HERE>\n",
        "%env SPEECH_KEY=<PUT_YOUR_API_KEY_HERE>\n",
        "%env SPEECH_REGION=<PUT_YOUR_REGION_HERE>\n",
        "%env DID_API_KEY=<PUT_YOUR_API_KEY_HERE>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
        "conversation = ConversationChain(\n",
        "    llm=OpenAI(max_tokens=2048, temperature=0.5),\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "def predict(input, history=[]):\n",
        "    history.append(input)\n",
        "    response = conversation.predict(input=input)\n",
        "    history.append(response)\n",
        "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "    return responses, history\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", container=False)\n",
        "\n",
        "    txt.submit(predict, [txt, state], [chatbot, state])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7863\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 10, in map_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 168, in start_tls\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 163, in start_tls\n",
            "    sock = ssl_context.wrap_socket(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1104, in _create\n",
            "    self.do_handshake()\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1375, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "ssl.SSLEOFError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 66, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 228, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 268, in handle_request\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 251, in handle_request\n",
            "    response = connection.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py\", line 317, in handle_request\n",
            "    stream = stream.start_tls(**kwargs)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 152, in start_tls\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 858, in _request\n",
            "    response = self._client.send(request, auth=self.custom_auth, stream=stream)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 901, in send\n",
            "    response = self._send_handling_auth(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 929, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 966, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 1002, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 227, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 83, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectError: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 10, in map_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 168, in start_tls\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 163, in start_tls\n",
            "    sock = ssl_context.wrap_socket(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1104, in _create\n",
            "    self.do_handshake()\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1375, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "TimeoutError: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 66, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 228, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 268, in handle_request\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 251, in handle_request\n",
            "    response = connection.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py\", line 317, in handle_request\n",
            "    stream = stream.start_tls(**kwargs)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 152, in start_tls\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectTimeout: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 858, in _request\n",
            "    response = self._client.send(request, auth=self.custom_auth, stream=stream)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 901, in send\n",
            "    response = self._send_handling_auth(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 929, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 966, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 1002, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 227, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 83, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectTimeout: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 10, in map_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 168, in start_tls\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 163, in start_tls\n",
            "    sock = ssl_context.wrap_socket(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1104, in _create\n",
            "    self.do_handshake()\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/ssl.py\", line 1375, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "TimeoutError: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 66, in map_httpcore_exceptions\n",
            "    yield\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 228, in handle_request\n",
            "    resp = self._pool.handle_request(req)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 268, in handle_request\n",
            "    raise exc\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py\", line 251, in handle_request\n",
            "    response = connection.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py\", line 317, in handle_request\n",
            "    stream = stream.start_tls(**kwargs)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_backends/sync.py\", line 152, in start_tls\n",
            "    with map_exceptions(exc_map):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
            "    raise to_exc(exc) from exc\n",
            "httpcore.ConnectTimeout: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 858, in _request\n",
            "    response = self._client.send(request, auth=self.custom_auth, stream=stream)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 901, in send\n",
            "    response = self._send_handling_auth(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 929, in _send_handling_auth\n",
            "    response = self._send_handling_redirects(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 966, in _send_handling_redirects\n",
            "    response = self._send_single_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_client.py\", line 1002, in _send_single_request\n",
            "    response = transport.handle_request(request)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 227, in handle_request\n",
            "    with map_httpcore_exceptions():\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/httpx/_transports/default.py\", line 83, in map_httpcore_exceptions\n",
            "    raise mapped_exc(message) from exc\n",
            "httpx.ConnectTimeout: _ssl.c:990: The handshake operation timed out\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/routes.py\", line 534, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/utils.py\", line 661, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/tmp/ipykernel_43451/2776339695.py\", line 32, in process_audio\n",
            "    text = transcribe(audio)\n",
            "  File \"/tmp/ipykernel_43451/2776339695.py\", line 28, in transcribe\n",
            "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/resources/audio/transcriptions.py\", line 99, in create\n",
            "    return self._post(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 1055, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 834, in request\n",
            "    return self._request(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 890, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 925, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 880, in _request\n",
            "    return self._retry_request(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 925, in _retry_request\n",
            "    return self._request(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/openai/_base_client.py\", line 887, in _request\n",
            "    raise APITimeoutError(request=request) from err\n",
            "openai.APITimeoutError: Request timed out.\n",
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "from langchain.llms import openai\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
        "conversation = ConversationChain(\n",
        "    llm=openai.OpenAI(max_tokens=2048, temperature=0.5),\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "def predict(input, history=[]):\n",
        "    history.append(input)\n",
        "    response = conversation.predict(input=input)\n",
        "    history.append(response)\n",
        "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "    return responses, history\n",
        "\n",
        "def transcribe(audio):\n",
        "    os.rename(audio, audio + '.wav')\n",
        "    audio_file = open(audio + '.wav', \"rb\")\n",
        "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
        "    return transcript.text\n",
        "\n",
        "def process_audio(audio, history=[]):\n",
        "    text = transcribe(audio)\n",
        "    return predict(text, history)\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:350px} .overflow-y-auto{height:500px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", container=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
        "\n",
        "    txt.submit(predict, [txt, state], [chatbot, state])\n",
        "    audio.change(process_audio, [audio, state], [chatbot, state])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/routes.py\", line 534, in predict\n",
            "    output = await route_utils.call_process_api(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/blocks.py\", line 1550, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/blocks.py\", line 1185, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2106, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 833, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/utils.py\", line 661, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/tmp/ipykernel_26096/1066729948.py\", line 45, in process_audio\n",
            "    text = transcribe(audio)\n",
            "  File \"/tmp/ipykernel_26096/1066729948.py\", line 39, in transcribe\n",
            "    os.rename(audio, audio + '.wav')\n",
            "TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "from langchain.llms import openai\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
        "conversation = ConversationChain(\n",
        "    llm=openai.OpenAI(max_tokens=2048, temperature=0.5),\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
        "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
        "\n",
        "# The language of the voice that speaks.\n",
        "speech_config.speech_synthesis_language='en-US'\n",
        "speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
        "\n",
        "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "def play_voice(text):\n",
        "    speech_synthesizer.speak_text_async(text)\n",
        "\n",
        "def predict(input, history=[]):\n",
        "    history.append(input)\n",
        "    response = conversation.predict(input=input)\n",
        "    history.append(response)\n",
        "    play_voice(response)\n",
        "    responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "    return responses, history\n",
        "\n",
        "def transcribe(audio):\n",
        "    os.rename(audio, audio + '.wav')\n",
        "    audio_file = open(audio + '.wav', \"rb\")\n",
        "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
        "    return transcript.text\n",
        "\n",
        "def process_audio(audio, history=[]):\n",
        "    text = transcribe(audio)\n",
        "    return predict(text, history)\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:800px} .overflow-y-auto{height:800px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", container=False)\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
        "\n",
        "    txt.submit(predict, [txt, state], [chatbot, state])\n",
        "    audio.change(process_audio, [audio, state], [chatbot, state])\n",
        "\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 'tlk_htrLTSm9tbxwlx6Zs2w6H', 'created_at': '2023-12-05T06:41:53.605Z', 'created_by': 'google-oauth2|103235674484908876863', 'status': 'created', 'object': 'talk'}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def generate_talk(input, avatar_url,\n",
        "                  voice_type = \"microsoft\",\n",
        "                  voice_id = \"en-US-JennyNeural\",\n",
        "                  api_key = os.environ.get('DID_API_KEY')):\n",
        "    url = \"https://api.d-id.com/talks\"\n",
        "    payload = {\n",
        "        \"script\": {\n",
        "            \"type\": \"text\",\n",
        "            \"provider\": {\n",
        "                \"type\": voice_type,\n",
        "                \"voice_id\": voice_id\n",
        "            },\n",
        "            \"ssml\": \"false\",\n",
        "            \"input\": input\n",
        "        },\n",
        "        \"config\": {\n",
        "            \"fluent\": \"false\",\n",
        "            \"pad_audio\": \"0.0\"\n",
        "        },\n",
        "        \"source_url\": avatar_url\n",
        "    }\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"authorization\": \"Basic \" + api_key\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "avatar_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/d18d8fff-453b-4fa4-b1e1-ab7972b80bb2/width=1344/00164-1714385542.jpeg\"\n",
        "text = \"Today is a cold and overcast day, I hope the sun comes out tomorrow.\"\n",
        "\n",
        "response = generate_talk(input=text, avatar_url=avatar_url)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'user': {'features': ['stitch', 'clips:write', None], 'stripe_plan_group': 'deid-trial', 'authorizer': 'basic', 'owner_id': 'google-oauth2|103235674484908876863', 'id': 'google-oauth2|103235674484908876863', 'plan': 'deid-trial', 'email': 'fenixping@gmail.com'}, 'script': {'length': 68, 'ssml': False, 'subtitles': False, 'type': 'text', 'provider': {'type': 'microsoft', 'voice_id': 'en-US-JennyNeural'}}, 'metadata': {'driver_url': 'bank://lively/driver-05/original', 'mouth_open': False, 'num_faces': 1, 'num_frames': 120, 'processing_fps': 41.16560407951136, 'resolution': [512, 512], 'size_kib': 1285.8515625}, 'audio_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103235674484908876863/tlk_htrLTSm9tbxwlx6Zs2w6H/microsoft.wav?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1701844913&Signature=pWjqriXSkoVeTdzyxLyuKZe5mjc%3D&X-Amzn-Trace-Id=Root%3D1-656ec62f-3723999a6f0208886d344a71%3BParent%3D4495df81583456b8%3BSampled%3D0%3BLineage%3Da08e19fe%3A0', 'created_at': '2023-12-05T06:41:53.605Z', 'face': {'mask_confidence': -1, 'detection': [490, 292, 827, 759], 'overlap': 'no', 'size': 671, 'top_left': [323, 190], 'face_id': 0, 'detect_confidence': 0.9998557567596436}, 'config': {'stitch': False, 'pad_audio': 0, 'align_driver': True, 'sharpen': True, 'reduce_noise': False, 'auto_match': True, 'normalization_factor': 1, 'show_watermark': True, 'motion_factor': 1, 'result_format': '.mp4', 'fluent': False, 'align_expand_factor': 0.3}, 'source_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103235674484908876863/tlk_htrLTSm9tbxwlx6Zs2w6H/source/00164-1714385542.jpeg?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1701844913&Signature=qPFzUo8qO2BnxaZp3OdjPn5dr%2Bc%3D&X-Amzn-Trace-Id=Root%3D1-656ec62f-3723999a6f0208886d344a71%3BParent%3D4495df81583456b8%3BSampled%3D0%3BLineage%3Da08e19fe%3A0', 'created_by': 'google-oauth2|103235674484908876863', 'status': 'done', 'driver_url': 'bank://lively/', 'modified_at': '2023-12-05T06:41:57.366Z', 'user_id': 'google-oauth2|103235674484908876863', 'subtitles': False, 'id': 'tlk_htrLTSm9tbxwlx6Zs2w6H', 'duration': 4.824, 'started_at': '2023-12-05T06:41:53.662', 'result_url': 'https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103235674484908876863/tlk_htrLTSm9tbxwlx6Zs2w6H/1701758513605.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1701844917&Signature=5ORBzuGOiJqBcRGHAcNC8RJGg5Q%3D&X-Amzn-Trace-Id=Root%3D1-656ec635-61db0b4e243366dc065fccc3%3BParent%3D7acbfc0449b7000e%3BSampled%3D1%3BLineage%3D6b931dd4%3A0'}\n"
          ]
        }
      ],
      "source": [
        "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
        "    url = \"https://api.d-id.com/talks/\" + id\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": \"Basic \"+api_key\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "talk = get_a_talk(response['id'])\n",
        "print(talk)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <video width=\"640\" height=\"480\" controls>\n",
              "        <source src=\"https://d-id-talks-prod.s3.us-west-2.amazonaws.com/google-oauth2%7C103235674484908876863/tlk_htrLTSm9tbxwlx6Zs2w6H/1701758513605.mp4?AWSAccessKeyId=AKIA5CUMPJBIK65W6FGA&Expires=1701844917&Signature=5ORBzuGOiJqBcRGHAcNC8RJGg5Q%3D&X-Amzn-Trace-Id=Root%3D1-656ec635-61db0b4e243366dc065fccc3%3BParent%3D7acbfc0449b7000e%3BSampled%3D1%3BLineage%3D6b931dd4%3A0\" type=\"video/mp4\">\n",
              "    Your browser does not support the video tag.\n",
              "    </video>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "def play_mp4_video(url):\n",
        "    video_tag = f\"\"\"\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "        <source src=\"{url}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>\n",
        "    \"\"\"\n",
        "    return HTML(video_tag)\n",
        "result_url = talk['result_url']\n",
        "play_mp4_video(result_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_43451/467225261.py:117: GradioUnusedKwargWarning: You have unused kwarg parameters in HTML, please remove them: {'live': False}\n",
            "  video = gr.HTML(f'<img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"John Carmack\">', live=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fw/miniconda3/envs/ai_aesthetics/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ],
      "source": [
        "import os, time, requests\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "from gradio import HTML\n",
        "from langchain.llms import openai\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=2048)\n",
        "conversation = ConversationChain(\n",
        "    llm=openai.OpenAI(max_tokens=2048, temperature=0.5),\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "avatar_url = \"https://image.civitai.com/xG1nkqKTMzGDvpLrqFT7WA/d18d8fff-453b-4fa4-b1e1-ab7972b80bb2/width=1344/00164-1714385542.jpeg\"\n",
        "\n",
        "def generate_talk(input, avatar_url,\n",
        "                  voice_type = \"microsoft\",\n",
        "                  voice_id = \"en-US-JennyNeural\",\n",
        "                  api_key = os.environ.get('DID_API_KEY')):\n",
        "    url = \"https://api.d-id.com/talks\"\n",
        "    payload = {\n",
        "        \"script\": {\n",
        "            \"type\": \"text\",\n",
        "            \"provider\": {\n",
        "                \"type\": voice_type,\n",
        "                \"voice_id\": voice_id\n",
        "            },\n",
        "            \"ssml\": \"false\",\n",
        "            \"input\": input\n",
        "        },\n",
        "        \"config\": {\n",
        "            \"fluent\": \"false\",\n",
        "            \"pad_audio\": \"0.0\"\n",
        "        },\n",
        "        \"source_url\": avatar_url\n",
        "    }\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"content-type\": \"application/json\",\n",
        "        \"authorization\": \"Basic \" + api_key\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "\n",
        "def get_a_talk(id, api_key = os.environ.get('DID_API_KEY')):\n",
        "    url = \"https://api.d-id.com/talks/\" + id\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"authorization\": \"Basic \"+api_key\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "\n",
        "def get_mp4_video(input, avatar_url=avatar_url):\n",
        "    response = generate_talk(input=input, avatar_url=avatar_url)\n",
        "    talk = get_a_talk(response['id'])\n",
        "    video_url = \"\"\n",
        "    index = 0\n",
        "    while index < 30:\n",
        "        index += 1\n",
        "        if 'result_url' in talk:\n",
        "            video_url = talk['result_url']\n",
        "            return video_url\n",
        "        else:\n",
        "            time.sleep(1)\n",
        "            talk = get_a_talk(response['id'])\n",
        "    return video_url\n",
        "\n",
        "def predict(input, history=[]):\n",
        "    if input is not None:\n",
        "        history.append(input)\n",
        "        response = conversation.predict(input=input)\n",
        "        video_url = get_mp4_video(input=response, avatar_url=avatar_url)\n",
        "        video_html = f\"\"\"<video width=\"320\" height=\"240\" controls autoplay><source src=\"{video_url}\" type=\"video/mp4\"></video>\"\"\"\n",
        "        history.append(response)\n",
        "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "        return responses, video_html, history\n",
        "    else:\n",
        "        video_html = f'<img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"Realistic style woman looking at viewer\">'\n",
        "        responses = [(u,b) for u,b in zip(history[::2], history[1::2])]\n",
        "        return responses, video_html, history\n",
        "\n",
        "def transcribe(audio):\n",
        "    os.rename(audio, audio + '.wav')\n",
        "    audio_file = open(audio + '.wav', \"rb\")\n",
        "    transcript = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)\n",
        "    return transcript.text\n",
        "\n",
        "def process_audio(audio, history=[]):\n",
        "    if audio is not None:\n",
        "        text = transcribe(audio)\n",
        "        return predict(text, history)\n",
        "    else:\n",
        "        text = None\n",
        "        return predict(text, history)\n",
        "\n",
        "with gr.Blocks(css=\"#chatbot{height:500px} .overflow-y-auto{height:500px}\") as demo:\n",
        "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        txt = gr.Textbox(show_label=False, placeholder=\"Enter text and press enter\", container=False)\n",
        "\n",
        "    with gr.Row():\n",
        "        audio = gr.Audio(source=\"microphone\", type=\"filepath\")\n",
        "\n",
        "    with gr.Row():\n",
        "        video = gr.HTML(f'<img src=\"{avatar_url}\" width=\"320\" height=\"240\" alt=\"Realistic style woman looking at viewer\">', live=False)\n",
        "\n",
        "    txt.submit(predict, [txt, state], [chatbot, video, state])\n",
        "    audio.change(process_audio, [audio, state], [chatbot, video, state])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai_aesthetics",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
